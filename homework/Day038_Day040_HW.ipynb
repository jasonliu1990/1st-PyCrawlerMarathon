{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 待更新\n",
    "    1. headers拿出來\n",
    "    2. 寫log\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cate_table(snap_date):\n",
    "    \n",
    "    \"\"\" 取得cate分類\n",
    "        1. cate_id\n",
    "        2. cate_title\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://today.line.me/TW/pc'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    cate = soup.select('._category a')\n",
    "    \n",
    "    cate_id = []\n",
    "    cate_name = []\n",
    "    for i in range(len(cate)):\n",
    "        tmp1 = re.sub('[a-zA-Z /]', '', cate[i].get('href'))\n",
    "        tmp2 = cate[i].get('title')\n",
    "        cate_id = cate_id + [tmp1]\n",
    "        cate_name = cate_name + [tmp2]\n",
    "        \n",
    "    cate = pd.DataFrame()\n",
    "    cate['cate_id'] = cate_id\n",
    "    cate['cate_name'] = cate_name\n",
    "    cate['snap_date'] = snap_date\n",
    "    print('cate table finished')\n",
    "   \n",
    "    return cate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_list(cate):\n",
    "    \n",
    "    cate_list = cate['cate_id'].values\n",
    "    url_main = 'https://today.line.me/TW/pc/main/'\n",
    "    url_main_list = []\n",
    "\n",
    "    for c in cate_list:\n",
    "        tmp = url_main + c\n",
    "        url_main_list.append(tmp)\n",
    "\n",
    "    url = []\n",
    "    url_list = []   \n",
    "    \n",
    "    for u in url_main_list:\n",
    "        try:\n",
    "            r = requests.get(u)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')   \n",
    "            tmp = list(map(lambda x: x.get('href'), soup.find_all('a')))\n",
    "            url.extend(tmp)\n",
    "        except:\n",
    "            print(u)\n",
    "            continue\n",
    "        \n",
    "    url_list = list(set(url))\n",
    "    print('cate url finished')\n",
    "\n",
    "    return url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_1(url_list, snap_date):\n",
    "\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36'}\n",
    "    tmp_dict_list = []\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            tmp_dict = {}\n",
    "            r = requests.get(url, headers= header)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            tmp_dict['title'] = soup.head.title.text\n",
    "            tmp_dict['author'] = soup.head.find_all('meta')[3].get('content')\n",
    "            tmp = soup.head.find_all('meta')[4].get('content').strip().split(',')\n",
    "            tmp_dict['title'] = tmp[1]\n",
    "            tmp_dict['post'] = tmp[2] + tmp[3]\n",
    "            content = list(map(lambda x : x.text, soup.select('.news-content p')))\n",
    "            tmp_dict['content'] = ' '.join(content)\n",
    "            tmp_dict['cate_id'] = soup.find_all('script')[1].text.split(';')[0].split('=')[1][2:-1]\n",
    "            tmp_dict['article_id'] = soup.find_all('script')[1].text.split(';')[2].split('=')[1][2:-1]\n",
    "            tmp_dict_list.append(tmp_dict)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    content_table = pd.DataFrame(tmp_dict_list)\n",
    "    content_table['article_id'] = content_table['article_id'].astype(str)\n",
    "    content_table['cate_id'] = content_table['cate_id'].astype(str)\n",
    "    content_table['snap_date'] = snap_date\n",
    "    print('content table finished')\n",
    "\n",
    "    \n",
    "    return content_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_2(content_table, snap_date):\n",
    "    \n",
    "    article_id = content_table['article_id'].values\n",
    "    com_url = 'https://api.today.line.me/webapi/comment/list?articleId=68687297&country=TW&replyCount=true&limit=100&direction=DESC&sort=POPULAR'\n",
    "    com_url_list = list(map(lambda x: com_url.replace('68687297',str(x)), article_id))\n",
    "    com_list = []\n",
    "    tmp = []\n",
    "    for u in com_url_list:\n",
    "        try:\n",
    "            j = 0\n",
    "            while True:\n",
    "\n",
    "                url = u + '&pivot=' + str(j) + '00'\n",
    "                r = requests.get(url)\n",
    "                tmp = json.loads(r.text)\n",
    "                com_list.append(tmp)\n",
    "\n",
    "                if tmp['result']['comments']['hasMore'] == True:\n",
    "\n",
    "                    j += 1\n",
    "\n",
    "                else :\n",
    "                    break\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    count_tmp = []\n",
    "    article_id_tmp = []\n",
    "    user_id_tmp = []\n",
    "    user_name_tmp = []\n",
    "    content_tmp = []\n",
    "    like_tmp = []\n",
    "    dislike_tmp = []\n",
    "    replyCount_tmp = []\n",
    "    commentSn_tmp = []\n",
    "\n",
    "    for num in range(len(com_list)):\n",
    "        try:\n",
    "            \"\"\" 解析json檔並提取所需資料\n",
    "            \"\"\"\n",
    "            cc = com_list[num]['result']['comments']['comments']\n",
    "            article_id = list(map(lambda x: x['articleId'],cc))\n",
    "            user_id = list(map(lambda x: x['pictureUrl'],cc))\n",
    "            user_name = list(map(lambda x: x['displayName'],cc))\n",
    "            content = list(map(lambda x: x['contents'][0]['extData']['content'],cc))\n",
    "            commentSn = list(map(lambda x: x['commentSn'],cc))\n",
    "\n",
    "            like = []\n",
    "            dislike = []\n",
    "            replyCount = []\n",
    "            \"\"\" like, dislike, replyCount要額外處理\n",
    "                抓到keyError就補0\n",
    "            \"\"\"\n",
    "            for rcc in com_list[num]['result']['comments']['comments']:\n",
    "\n",
    "                try:    \n",
    "                    like = rcc['ext']['likeCount']['up']\n",
    "                    like_tmp.append(like) \n",
    "                except KeyError:\n",
    "                    like_tmp.append(0) \n",
    "\n",
    "                try:\n",
    "                    dislike = rcc['ext']['likeCount']['down']\n",
    "                    dislike_tmp.append(dislike)\n",
    "                except KeyError:\n",
    "                    dislike_tmp.append(0)\n",
    "\n",
    "                try:\n",
    "                    replyCount = rcc['ext']['replyCount']\n",
    "                    replyCount_tmp.append(replyCount)\n",
    "                except KeyError:  \n",
    "                    replyCount_tmp.append(0)\n",
    "\n",
    "            article_id_tmp.extend(article_id)\n",
    "            user_id_tmp.extend(user_id)\n",
    "            user_name_tmp.extend(user_name)\n",
    "            content_tmp.extend(content)\n",
    "            commentSn_tmp.extend(commentSn)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    tmp_dict = { 'article_id' : article_id_tmp,\n",
    "                 'user_id' : user_id_tmp,\n",
    "                 'user_name' : user_name_tmp,\n",
    "                 'comment' : content_tmp,\n",
    "                 'like' : like_tmp,\n",
    "                  'dislike' : dislike_tmp,\n",
    "                  'replyCount' : replyCount_tmp,\n",
    "                 'commentSn' : commentSn_tmp\n",
    "               }\n",
    "\n",
    "    comment_table = pd.DataFrame(tmp_dict)\n",
    "    comment_table['comment_id'] = comment_table['article_id'] + comment_table['commentSn']\n",
    "    comment_table['snap_date'] = snap_date\n",
    "\n",
    "    print('comment table finished')\n",
    "\n",
    "    return comment_table, com_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_3(comment_table, snap_date):\n",
    "    \n",
    "    \"\"\" 利用文章id和留言流水號建立回文的url\n",
    "        來找出有reply的網址減少爬蟲時間\n",
    "    \"\"\"\n",
    "    url = 'https://api.today.line.me/webapi/comment/list?articleId=%s&limit=100&country=TW&parentCommentSn=%s'\n",
    "    filter = comment_table['replyCount'] > 0\n",
    "    reply_id = (comment_table[filter]['article_id'] + ',' + comment_table[filter]['commentSn']).values\n",
    "    reply_url_list = list(map(lambda x: url%tuple(x.split(',')), reply_id))\n",
    "\n",
    "    reply_list = []\n",
    "    for url in reply_url_list:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            soup = json.loads(r.text)\n",
    "            reply_list.append(soup)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    article_id_tmp = []\n",
    "    commentSn_tmp = []\n",
    "    user_name_tmp = []\n",
    "    user_id_tmp = []\n",
    "    comment_tmp = []\n",
    "\n",
    "    for i in range(len(reply_list)):\n",
    "        try:      \n",
    "            cc = reply_list[i]['result']['comments']['comments']\n",
    "            article_id = list(map(lambda x: x['articleId'], cc))\n",
    "            commentSn = list(map(lambda x: x['commentSn'], cc))\n",
    "            user_name = list(map(lambda x: x['displayName'], cc))\n",
    "            user_id = list(map(lambda x: x['pictureUrl'], cc))\n",
    "            comment = list(map(lambda x: x['contents'][0]['extData']['content'], cc))\n",
    "\n",
    "            article_id_tmp.extend(article_id)\n",
    "            commentSn_tmp.extend(commentSn)\n",
    "            user_name_tmp.extend(user_name)\n",
    "            user_id_tmp.extend(user_id)\n",
    "            comment_tmp.extend(comment)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "    tmp_dict = {'article_id' : article_id_tmp,\n",
    "                'commentSn' : commentSn_tmp,\n",
    "                'user_id' : user_id_tmp,\n",
    "                'user_name' : user_name_tmp,\n",
    "                'comment' : comment_tmp\n",
    "                }\n",
    "\n",
    "    reply_table = pd.DataFrame(tmp_dict)\n",
    "    reply_table['comment_id'] = reply_table['article_id'] + reply_table['commentSn']\n",
    "    reply_table['snap_date'] = snap_date\n",
    "    print('reply table finished')\n",
    "    \n",
    "    return reply_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cate table finished\n",
      "cate url finished\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "snap_date = datetime.datetime.strftime(start, '%Y%m%d')\n",
    "\n",
    "cate = create_cate_table(snap_date)\n",
    "url_list = create_url_list(cate)\n",
    "content_table = create_table_1(url_list, snap_date)\n",
    "comment_table, com_list = create_table_2(content_table, snap_date)\n",
    "reply_table = create_table_3(comment_table, snap_date)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content shape:(1531, 7)\n",
      "comment shape:(18305, 10)\n",
      "reply shape:(6506, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f'content shape:{content_table.shape}')\n",
    "print(f'comment shape:{comment_table.shape}')\n",
    "print(f'reply shape:{reply_table.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSSQL\n",
    "* pip install sqlalchemy\n",
    "* pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mssql+pyodbc://sa:password@localhost:1433/linetoday?driver=SQL+Server+Native+Client+11.0\", echo=False)\n",
    "cate.to_sql('cate', engine, index=False, if_exists='append')\n",
    "reply_table.to_sql('reply', engine, index=False, if_exists='append')\n",
    "comment_table.to_sql('comment', engine, index=False, if_exists='append')\n",
    "content_table.to_sql('content', engine, index=False, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'pyscraper'",
   "language": "python",
   "name": "pyscraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
